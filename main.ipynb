{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the headers for the CSV file\n",
    "HEADERS = ['title', 'type', 'address']  # default\n",
    "\n",
    "# Dynamic variable for file name for saving the HTML markup text to save and csv data\n",
    "MARKUP_FILENAME = 'html-markup.txt'\n",
    "CSV_DATA_FILENAME = 'data.csv'\n",
    "\n",
    "# Set the url you want to your own 2GIS url for which you want to get the data. Below mentioned is an example url from 2GIS\n",
    "URL = 'https://2gis.ru/kazan/search/%D0%B6%D0%BA?m=49.255846%2C55.793323%2F10.89'\n",
    "\n",
    "# Writing the headers to the CSV file\n",
    "with open(CSV_DATA_FILENAME, 'w', encoding='utf-8', newline='') as f:\n",
    "\n",
    "    csv_writer = csv.DictWriter(f, fieldnames=HEADERS)\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "def data_helper(markup_data, headers_list):\n",
    "    div_data_titles = markup_data.find_all('div', class_='_1hf7139')\n",
    "    for idx, div in enumerate(div_data_titles):\n",
    "        if '_oqoid' not in str(div):\n",
    "            headers_list[1].insert(idx, 'NA')\n",
    "        elif '_tluih8' not in str(div):\n",
    "            headers_list[2].insert(idx, 'NA')\n",
    "    \n",
    "\n",
    "# This function basically reads the HTML file data from UI and uses its markup data for the beautifulSoup as an argument\n",
    "def data_handler(page):\n",
    "    with open(MARKUP_FILENAME, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    doc = BeautifulSoup(contents, features='html5lib')  # The <doc> variable holds the markup data read from the HTML file\n",
    "\n",
    "    titles = [k.text for k in doc.find_all('span', class_='_hc69qa')]\n",
    "\n",
    "    types = [k.text for k in doc.find_all('span', class_='_oqoid')]\n",
    "\n",
    "    addresses = [k.text for k in doc.find_all('span', class_='_tluih8')]\n",
    "\n",
    "    # Checking for not-provided data\n",
    "    if len(titles)!=len(types) or len(titles)!=len(addresses):\n",
    "        data_helper(doc, [titles, types, addresses])\n",
    "    # csv data row writing\n",
    "    with open(CSV_DATA_FILENAME, 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for m in range(1, len(titles)):\n",
    "            writer.writerow([titles[m], types[m], addresses[m]])\n",
    "\n",
    "    print(f'\\nfinished parsing page: {page}')  # Gives the status for each page if the parsing is done\n",
    "\n",
    "browser = webdriver.Chrome()  # Setting the webdriver for chrome\n",
    "\n",
    "# Maximazing allows the parser configurations to work more smoothly for avoiding 'no element exceptions'\n",
    "browser.maximize_window()\n",
    "\n",
    "browser.get(URL)  # opening the url via webdriver\n",
    "\n",
    "# Implicitly waiting for the page to load its contents in seconds\n",
    "browser.implicitly_wait(10)  # customize the time if your internet is slow\n",
    "\n",
    "page_element = browser.find_element(\n",
    "    By.XPATH, \"(//span[@class='_18lf326a'])[1]\")\n",
    "\n",
    "num_of_pages = (int(page_element.text)//12)+3  # calculates the number of pages to click\n",
    "\n",
    "try:\n",
    "    # This is the main loop and it works on page limitition of the corresponding data search passed to the webdriver\n",
    "    for page in range(1, num_of_pages):\n",
    "        with open(MARKUP_FILENAME, 'w', encoding='utf-8') as f:\n",
    "            f.write(browser.page_source)\n",
    "\n",
    "        data_handler(page)\n",
    "\n",
    "        time.sleep(1.9)\n",
    "\n",
    "        # getting the scroll element in DOM for getting the whole HTML markup so that beautifulSoup can parse it according to the headers list config\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[3]\")\n",
    "\n",
    "        # scrolling the <ul></ul> element\n",
    "        browser.execute_script(\"arguments[0].scrollIntoView(false);\", scroll_container)\n",
    "\n",
    "        # clicking on the next page DOM element\n",
    "        browser.find_element(By.XPATH, \"//div[@class='_5ocwns']//div[2]\").click()\n",
    "except IndexError:  # This error is triggered when the script crosses the page limit in the UI \n",
    "    print(f'Total pages parsed {page-1}')\n",
    "    time.sleep(3)\n",
    "    browser.quit()  # Exiting the driver runned Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicate entries in the data CSV file if any, optional code block\n",
    "df = pd.read_csv(CSV_DATA_FILENAME)\n",
    "prev = df.shape[0]\n",
    "# Dropping duplicates inplace so as not to make any copies of the original CSV data\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(CSV_DATA_FILENAME, index=False)  # re-writing the cleaned data to the original file\n",
    "print(f'\\nduplicates dropped: {prev-df.shape[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263b2455bde33485997a3dbeae7eefc58c037db36f19208f2d2feb2b480f596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
