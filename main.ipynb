{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the headers for the CSV file\n",
    "headers = ['title', 'type', 'address']  # default\n",
    "\n",
    "# dynamic variable for file name for saving the HTML markup text to save and csv data\n",
    "markup_fileName = 'html-markup.txt'\n",
    "csv_data_fileName = 'data.csv'\n",
    "\n",
    "# writing the headers to the CSV file\n",
    "with open(csv_data_fileName, 'w', encoding='utf-8', newline='') as f:\n",
    "\n",
    "    csv_writer = csv.DictWriter(f, fieldnames=headers)\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "def data_helper(markup_data, headers_list):\n",
    "    div_data_titles = markup_data.find_all('div', class_='_1hf7139')\n",
    "    for idx, div in enumerate(div_data_titles):\n",
    "        if '_oqoid' not in str(div):\n",
    "            headers_list[1].insert(idx, 'NA')\n",
    "        elif '_tluih8' not in str(div):\n",
    "            headers_list[2].insert(idx, 'NA')\n",
    "    \n",
    "\n",
    "# this function basically reads the file data and uses its markup data for the beautifulSoup as an argument\n",
    "def data_handler(page):\n",
    "    with open(markup_fileName, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    doc = BeautifulSoup(contents, features='html5lib')  # this doc holds the markup data read from the file\n",
    "\n",
    "    titles = [k.text for k in doc.find_all('span', class_='_hc69qa')]\n",
    "\n",
    "    types = [k.text for k in doc.find_all('span', class_='_oqoid')]\n",
    "\n",
    "    addresses = [k.text for k in doc.find_all('span', class_='_tluih8')]\n",
    "\n",
    "    # checking for not-provided data\n",
    "    if len(titles)!=len(types) or len(titles)!=len(addresses):\n",
    "        data_helper(doc, [titles, types, addresses])\n",
    "    # csv data row writing\n",
    "    with open(csv_data_fileName, 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for m in range(1, len(titles)):\n",
    "            writer.writerow([titles[m], types[m], addresses[m]])\n",
    "\n",
    "    print(f'\\nfinished parsing page: {page}')  # gives the status for each page if the parsing is done\n",
    "\n",
    "browser = webdriver.Chrome()  # setting the webdriver for chrome\n",
    "\n",
    "# maximazing allows the parser configurations to work more smoothly for avoiding 'no element exceptions'\n",
    "browser.maximize_window()\n",
    "\n",
    "# set the url you want to your own 2gis url for which you want to get the data\n",
    "URL = 'https://2gis.ru/kazan/search/%D0%B6%D0%BA?m=49.255846%2C55.793323%2F10.89'\n",
    "\n",
    "browser.get(URL)  # opening the url via webdriver\n",
    "\n",
    "# implicitly waiting for the page to load its contents in seconds\n",
    "browser.implicitly_wait(10)  # customize the time if your internet is slow\n",
    "\n",
    "page_element = browser.find_element(\n",
    "    By.XPATH, \"(//span[@class='_18lf326a'])[1]\")\n",
    "\n",
    "num_of_pages = (int(page_element.text)//12)+3  # calculates the number of pages to click\n",
    "\n",
    "try:\n",
    "    # main loop, it works on page limitition of the corresponding data search passed to the webdriver\n",
    "    for page in range(1, num_of_pages):\n",
    "        with open(markup_fileName, 'w', encoding='utf-8') as f:\n",
    "            f.write(browser.page_source)\n",
    "\n",
    "        data_handler(page)\n",
    "\n",
    "        time.sleep(1.9)\n",
    "\n",
    "        # getting the scroll element in DOM for getting the whole HTML markup so that beautifulSoup can parse it according to the headers list config\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[3]\")\n",
    "\n",
    "        # scrolling the <ul></ul> element\n",
    "        browser.execute_script(\"arguments[0].scrollIntoView(false);\", scroll_container)\n",
    "\n",
    "        # clicking on the next page DOM element\n",
    "        browser.find_element(By.XPATH, \"//div[@class='_5ocwns']//div[2]\").click()\n",
    "except IndexError:\n",
    "    print(f'Total pages parsed {page-1}')\n",
    "    time.sleep(3)\n",
    "    browser.quit()  # exitting the driver runned Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicate entries in the data CSV file if any, optional code block\n",
    "df = pd.read_csv(csv_data_fileName)\n",
    "prev = df.shape[0]\n",
    "# dropping duplicates inplace so as not to make any copies of the original CSV data\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(csv_data_fileName, index=False)  # re-writing the cleaned data to the original file\n",
    "print(f'\\nduplicates dropped: {prev-df.shape[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263b2455bde33485997a3dbeae7eefc58c037db36f19208f2d2feb2b480f596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
