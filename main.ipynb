{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lib imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for CSV file\n",
    "HEADERS = ['title', 'type', 'address', 'rating']  # default\n",
    "\n",
    "MARKUP_FILENAME = 'html-markup.txt'\n",
    "CSV_DATA_FILENAME = 'data.csv'\n",
    "\n",
    "# Set the city name, search to your own for which you want to get the data. Below mentioned is an example\n",
    "CITY_NAME = 'Moscow'\n",
    "SEARCH = 'hotels'\n",
    "\n",
    "URL = f'https://2gis.ru/{CITY_NAME.lower()}/search/{SEARCH}?m'\n",
    "# URL = 'https://2gis.ru/kazan/search/%D0%B6%D0%BA?m=49.255846%2C55.793323%2F10.89'\n",
    "\n",
    "# Writes the headers to the CSV file\n",
    "with open(CSV_DATA_FILENAME, 'w', encoding='utf-8', newline='') as f:\n",
    "\n",
    "    csv_writer = csv.DictWriter(f, fieldnames=HEADERS)\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "def extract_content(markup_text):\n",
    "    sub_doc = BeautifulSoup(markup_text, 'html.parser')\n",
    "    addresses = [k for k in sub_doc.find_all('span', class_='_14quei')]\n",
    "\n",
    "    if addresses:\n",
    "        x = re.findall('(<span class=\"_1w9o2igt\">(.|\\n)*?<\\/span>)', str(addresses[0]))\n",
    "        \n",
    "        if x:\n",
    "            return BeautifulSoup(str(x[0][0] if x[0][0] else x[0]), 'html.parser').text\n",
    "        else:\n",
    "            return 'null'\n",
    "    else:\n",
    "        return 'null'\n",
    "\n",
    "# Reads the HTML file data from UI\n",
    "def data_handler(page_number):\n",
    "    with open(MARKUP_FILENAME, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    doc = BeautifulSoup(contents, features='html.parser')  # The <doc> variable holds the markup data read from the HTML file\n",
    "\n",
    "    parent_div_results = doc.find_all('div', class_='_1kf6gff')\n",
    "\n",
    "    TITLES = []\n",
    "    TYPES = []\n",
    "    ADDRESSES = []\n",
    "    RATINGS = []\n",
    "\n",
    "    for idx, res_div in enumerate(parent_div_results):\n",
    "\n",
    "        type = res_div.find('span', class_='_oqoid').text if res_div.find('span', class_='_oqoid') else 'null'\n",
    "        title = res_div.find('span', class_='_1al0wlf').text if res_div.find('span', class_='_1al0wlf') else type\n",
    "        address = extract_content(str(res_div.find('span', class_='_14quei')))\n",
    "        rating = res_div.find('div', class_='_y10azs').text if res_div.find('div', class_='_y10azs') else 'null'\n",
    "\n",
    "        # Appending each page data to their corresponding lists\n",
    "        TITLES.append(title)\n",
    "        TYPES.append(type)\n",
    "        ADDRESSES.append(address)\n",
    "        RATINGS.append(rating)\n",
    "\n",
    "\n",
    "    # # Writes row data for current page\n",
    "    with open(CSV_DATA_FILENAME, 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for m in range(1, len(TITLES)):\n",
    "            writer.writerow([TITLES[m], TYPES[m], ADDRESSES[m], RATINGS[m]])\n",
    "\n",
    "    print(f'\\nfinished parsing page: {page_number}')  # Logger for parsing status\n",
    "\n",
    "\n",
    "# Webdriver for chrome\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# Maximazing allows the parser configurations to work more smoothly for avoiding 'no element exceptions'\n",
    "browser.maximize_window()\n",
    "\n",
    "# Opens the url using webdriver\n",
    "browser.get(URL)\n",
    "\n",
    "# Implicitly waiting for the page to load its contents in seconds\n",
    "# Customize the time if your internet is slow\n",
    "browser.implicitly_wait(40)\n",
    "\n",
    "\n",
    "# page_element = browser.find_element(\n",
    "#     By.XPATH, \"(//span[@class='_18lf326a'])[1]\")\n",
    "page_element = browser.find_element(\n",
    "    By.XPATH, \"(//span[@class='_1xhlznaa'])[1]\")\n",
    "\n",
    "num_of_pages = (int(page_element.text)//12)+3  # Calculates the number of pages\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    # Accessing the scroll element in DOM to get the whole HTML markup so that beautifulSoup can parse it according to the headers list config\n",
    "    try:\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[3]\")\n",
    "    except NoSuchElementException:\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[2]\")\n",
    "    finally:\n",
    "        # Parses and navigates through all the pages\n",
    "        for page in range(1, num_of_pages):\n",
    "            with open(MARKUP_FILENAME, 'w', encoding='utf-8') as f:\n",
    "                f.write(browser.page_source)\n",
    "\n",
    "            # Handles the parsing and writing of CSV data\n",
    "            data_handler(page)\n",
    "\n",
    "\n",
    "            time.sleep(2)\n",
    "            # Scrolls the <ul></ul> element\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView(false);\", scroll_container)\n",
    "\n",
    "            time.sleep(1)\n",
    "            # Clicks on the next page <DOM element>\n",
    "            browser.find_element(By.XPATH, \"//div[@class='_5ocwns']//div[2]\").click()\n",
    "\n",
    "except IndexError:  # This error is triggered when the script crosses the page limit in the UI\n",
    "    print(f'Total pages parsed {page-1}')\n",
    "    time.sleep(3)\n",
    "finally:\n",
    "    time.sleep(2)\n",
    "    browser.quit()  # Exiting the Chrome driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete duplicates if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "duplicates dropped: 11\n"
     ]
    }
   ],
   "source": [
    "# Deleting duplicate entries in the data CSV file if any, optional code block\n",
    "df = pd.read_csv(CSV_DATA_FILENAME)\n",
    "\n",
    "prev = df.shape[0]\n",
    "# # Dropping duplicates inplace so as not to make any copies of the original CSV data\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(CSV_DATA_FILENAME, index=False)  # re-writing the cleaned data to the original file\n",
    "print(f'\\nduplicates dropped: {prev-df.shape[0]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263b2455bde33485997a3dbeae7eefc58c037db36f19208f2d2feb2b480f596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
