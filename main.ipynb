{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the headers for the CSV file\n",
    "headers = ['title', 'type', 'address']  # default\n",
    "\n",
    "# dynamic variable for file name for saving the HTML markup text to save\n",
    "fileName = 'html-markup.txt'\n",
    "\n",
    "# writing the headers to the CSV file\n",
    "with open('data.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "\n",
    "    csv_writer = csv.DictWriter(f, fieldnames=headers)\n",
    "\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "# this function basically reads the file data and uses its markup data for the beautifulSoup as an argument\n",
    "def scrape_data(page):\n",
    "    with open(fileName, 'r', encoding='utf-8') as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    doc = BeautifulSoup(contents, features='html5lib')  # this doc holds the markup data read from the file\n",
    "\n",
    "    titles = [k.text for k in doc.find_all('span', class_='_hc69qa')]\n",
    "\n",
    "    types = [k.text for k in doc.find_all('span', class_='_oqoid')]\n",
    "\n",
    "    addresses = [k.text for k in doc.find_all('span', class_='_tluih8')]\n",
    "\n",
    "    # csv data row writing\n",
    "    with open('data.csv', 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for m in range(1, len(titles)):\n",
    "            writer.writerow([titles[m], types[m], addresses[m]])\n",
    "\n",
    "    print(f'\\nfinished parsing {page}')  # gives the status for each page if the parsing is done\n",
    "\n",
    "browser = webdriver.Chrome()  # setting the webdriver for chrome\n",
    "\n",
    "# maximazing allows the parser configurations to work more smoothly for avoiding 'no element exceptions'\n",
    "browser.maximize_window()\n",
    "\n",
    "# set the url you want to your own 2gis url for which you want to get the data\n",
    "url = 'https://2gis.ru/kazan/search/%D1%81%D1%82%D1%80%D0%BE%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5%20%D0%BA%D0%BE%D0%BC%D0%BF%D0%B0%D0%BD%D0%B8%D0%B8/rubricId/298?m=49.184037%2C55.784963%2F11.47'\n",
    "\n",
    "browser.get(url)  # opening the url via webdriver\n",
    "\n",
    "# implicitly waiting for the page to load its contents in seconds\n",
    "browser.implicitly_wait(10)  # customize the time if your internet is slow\n",
    "\n",
    "page_element = browser.find_element(\n",
    "    By.XPATH, \"(//span[@class='_18lf326a'])[1]\")\n",
    "\n",
    "num_of_pages = (int(page_element.text)//12)+3  # calculates the number of pages to click\n",
    "\n",
    "# main loop, it works on page limitition of the corresponding data search passed to the webdriver\n",
    "for page in range(1, num_of_pages):\n",
    "    with open(fileName, 'w', encoding='utf-8') as f:\n",
    "        f.write(browser.page_source)\n",
    "\n",
    "    scrape_data(page)\n",
    "\n",
    "    time.sleep(1.8)\n",
    "\n",
    "    # getting the scroll element in DOM for getting the whole HTML markup so that beautifulSoup can parse it according to the headers list config\n",
    "    scroll_container = browser.find_element(\n",
    "        By.XPATH, \"(//div[@class='_15gu4wr'])[3]\")\n",
    "\n",
    "    # scrolling the <ul></ul> element\n",
    "    browser.execute_script(\"arguments[0].scrollIntoView(false);\", scroll_container)\n",
    "\n",
    "    # clicking on the next page DOM element\n",
    "    browser.find_element(By.XPATH, \"//div[@class='_5ocwns']//div[2]\").click()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c263b2455bde33485997a3dbeae7eefc58c037db36f19208f2d2feb2b480f596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
